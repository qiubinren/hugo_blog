---
title: 大数据话单排重任务
date: 2017-02-07 21:07:51
draft: false
tags: ["Bigdata", "Hbase", "Coherence"]
categories: ["Bigdata", "Hbase", "Coherence"]
---
# 大数据话单排重任务
## 起因
昨天被领导拉去会议室，让帮忙抽空研究下mongoDB和tair两种key/value存储系统。简单地跟我讲了下要处理的问题，很多细节并没有和我说的很详细，主要的任务也只是想让我帮忙研究下两种存储系统的特点之后给他们汇报下，我就一知半解地接了这么个学习任务，所以之后博客开始会写点这方面的内容。
## 问题描述
江苏移动每天会产生大概40亿的话单量（数据存疑，领导当时随口跟我一说，我没听清楚，反正数据不小），这些话单里会存在一些重复话单（重复的话单不会很多，但是时间在两个月范围内的话单都可能重复，说实话计费这边的业务接触的很少，这里让我不太理解，上游给我们的话单为啥时间跨度这么大），这个时候就面临了排重的问题，现网的做法是直接插入oracle数据库（大概是做了很多分表和分库吧），让唯一索引来控制话单的唯一性，比较简单，速度不理想。现在计费系统云化之后，也希望用新技术来优化这个问题的解决方案。

<!--more-->

## 大体草案
目前部门里好像有两种方案（这边写好像是因为确实没和我细说，这边的内容是我根据了解的信息推测的,所以以下方案描述都是我的理解，很可能存在“皇帝用金扁担干活”这种笑话，根本和部门大佬们的讨论不沾边）。  
第一种方案好像是这样子的，最理想的方案就是全放分布式内存数据库里，话单几个关键字组成个key，查询后写入。但是呢，数据量毕竟太大了，一天就是40亿，要存起码两个月的数据量，肯定不能给这么多内存。然后呢，顺其自然地想内存存一部分数据（redis之类的技术)，其他的数据存文件里(HBase之类的)，然后话单优先到内存中排重，若发现找不到，再去文件里排一次，都找不到再插入。  
这个方案很明显有点问题，这个场景其实是不存在热点数据这种概念的，并不能知道哪些话单key是热点数据，直接在第一层就给排除出去，而且吧，大部分话单其实是不重复话单，所以这个缓存加的显得很多余，给我的感觉还不如直接上HBase，省去所有话单内存查一遍这一步。  
然后呢，就说内存里放这一两天的话单，文件系统里放之前的话单。来的话单先判时间，时间是这一两天的，直接在内存里做，时间不是这一两天的，在文件系统里做，这个方案听上去就靠谱多了，不用考虑数据内存淘汰机制，内存里有的KEY，文件系统也不用有了，等换天的时候再同步。如果说过来的话单大部分集中在这一两天，只有很少一部分是其他时间的话，那感觉很靠谱。但是这个方案还是有两点疑点：  
>1、过来的话单时间的规则到底是不是这种设想的情况呢？（业务不熟，无法了解需求的全部细节）  
>2、分布式内存数据库放这一两天的话单到底需要多少内存？数据量大到这种程度性能到底如何？

万一来的话单数量集中的时间不是这么个规则，或者分配的内存里干脆连这点key也存不下，那么花这么大的精力搞了这么大块内存，把话单数据放进去，大部分操作还是在文件系统里做，就又显得多余了。

第二种方案是部门大佬，想直接用淘宝开源的tair来做，这个说实话，我还是第一次听说这个东西，第二种方案领导说的比第一种信息要少很多，只有一个是明确的，就是用tair，这边说下我大概看了下后的猜测，tair三种存储模式，rdb,mdb,ldb，rdb其实就是redis纯内存，mdb是redis带持久化，然后ldb就是google开源的levelDB，给我的感觉这个tair就是数据库界的百度全家桶，把常见nosql给打包搞了个集群。感觉大佬其实思路还是前面的缓存+文件的思路，缓存数据就放在rdb或者mdb里，持久化用ldb，这边说下levelDB的特性，对磁盘写优化，效率高，随机读效率低，具体的内部机制其实和HBase差不多，只是HBase是分布式的，levelDB是单机的。猜测大佬可能是想自己改代码来确定缓存数据淘汰机制？？？

## 我的理解
很明显，我对业务和相关技术很不熟悉，所以以上这些猜测，很大可能是瞎逼逼。这些其实也不是领导给我的任务，领导给我的任务其实很简单，就是工作之余，抽一点时间出来学习下mongoDB和tair，研究下，给他们汇报下。不过我怎么说也是一只在校玩过算法，参加过ACM的小菜鸡，遇到这种问题习惯性想要找到最优方案，想得比较多，但是无论是技术层面的了解还是业务层面的了解都限制了我的视野，一切的想法其实还都只是空想。  
其实这次的问题，给我的感觉和一道经典面试题挺像的，就是爬虫的海量url排重，爬虫为了避免重复爬取网页，形成环路，每次爬取网页之前也要进行这么一个海量url排重的步骤，两个问题面对的问题其实是有相似的地方的。差别主要在准确性上，爬虫少爬或者多爬两个网页其实没啥大问题，话单排重有重复或者少话单了，对用户肯定是有影响的，有被投诉的风险。
