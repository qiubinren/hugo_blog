---
title: "深入理解Docker技术本质"
date: 2018-10-03T17:43:40+08:00
draft: false
tags: ["Docker", "Container", "Kubernetes"]
categories: ["Docker", "Container", "Kubernetes"]
---

# 深入理解Docker技术本质

## 本文缘起

本文本来是自己的学习笔记，后来又做成了给部门同事培训用的`ppt`，内含两次培训内容，所以内容有点长。这边推荐一下[极客时间](https://time.geekbang.org/)张磊老师的[深入剖析Kubernetes](https://time.geekbang.org/column/intro/116)系列课程，真是好课。

<!--more-->

## 带着问题看容器技术

什么是容器？网上经常可以看到这幅图。

![p1](/img/2018-10-03-p1.png)

网上会告诉你容器是用于解决应用打包部署问题，会把容器和虚拟化技术比较，一般会谈到给每个进程划分独立空间的思想，然后告诉你容器技术其实是一种沙盒技术，沙盒技术就是把应用隔离起来，互不干扰。

真的是这样吗？

有没有过好奇，再轻量级的虚拟化技术，总会有一个拦截模拟的中间件，对运行信息的动态修改，包括`cpu`，内存，磁盘，网络。。。这么复杂的模拟怎么能够有网上吹嘘的一台主机跑`10000`个容器呢？

真的不是吹牛么？这是什么黑科技！！！

## 从进程说起

容器到底怎么回事？

首先来看沙盒的本质能力：创造一个“隔离和限制”的边界。

这很好理解，沙盒内的应用只能看到沙盒内的应用，看不到其他沙盒，也看不到外面，这是隔离。

沙盒内的应用也只能使用沙盒给定的`cpu`资源，内存资源等系统资源，不能使用沙盒外的系统资源，也不能使用其他沙盒的系统资源，这是限制。

使用技术手段实现这两个能力就可以视为实现了容器。

那就来谈谈实现边界的手段，看看我们要隔离的对象——进程。

进程大家写了这么多了，也比较熟了。它的静态表现就是程序，平常安安静静呆在磁盘上，而一旦运行起来，它就变成计算机里数据和状态的总和，这就是它的动态表现。

约束和修改进程的动态表现，创造一个“边界”的存在，那就可以认为已经实现了容器。

## 看看docker如何实现“边界"

对于大多数`linux`容器技术，`Cgroups`技术是制造约束的手段，`Namespace`技术是修改进程视图的主要方法。

是不是觉得很抽象？不要急，我们慢慢来看。

首先我们创建一个容器来看看（即大名鼎鼎的`docker run`命令）：

``` shell
docker run -ti 172.32.150.63/jsnl/busybox:1.25.0 /bin/sh
```

这个命令很多人用的也挺熟了，这边还是解释一下。`-ti`参数是告诉`docker`项目，容器启动后给我们分配一个文本输入输出环境，方便我们跟容器交互。 `172.32.150.63/jsnl/busybox:1.25.0`是我要启动的容器镜像名字和版本号，`/bin/sh`是我们要在`docker`容器里运行的程序。

执行一下，进入一个控制台，这个时候玩过`docker`的肯定不会感到陌生，我们`ps`一下，有趣的事情发生了。

``` shell
[root@vcapp63 ~]# docker run -ti 172.32.150.63/jsnl/busybox:1.25.0 /bin/sh
/ # ps
PID   USER     TIME   COMMAND
1 root       0:00 /bin/sh
7 root       0:00 ps
```

`/bin/sh`我们刚才指定在容器内运行的这个程序，`pid`是`1`，而且整个容器内只有两个进程，可以看到宿主机上的进程全部都不见了。

本来我们在宿主机上运行`/bin/sh`，操作系统会给它分配一个编号，比如`pid=100`，而`pid=1`的进程肯定是统领全局的调度进程。但是通过`docker`执行`/bin/sh`后，`docker`给它施展了一个“障眼法”，让它看不到前面`99`个进程，自以为自己是第一个。这种机制就是在应用隔离的进程空间上做手脚，使得它只能看到重新计算后的进程编号。实际上，它在宿主机上仍然是原来的第`100`号进程。

这种技术就是`Linux`操作系统自带的`Namespace`技术。

## 理解Namespace

`Namespace`的实现其实很有意思，它其实是`Linux`创建新进程的一个参数。

系统调用是哪个？创建新进程写过或者看过公司`c++`系统代码的可能会脱口而出`fork`，其实还有个更自由的`clone`。

``` c
int pid = clone(main_function, stack_size, SIGCHLD, NULL); 
```

这个系统调用，就是创建一个新进程，并且返回它的`pid`。在创建新进程的时候，我们就可以在参数里指定`CLONE_NEWPID`参数，比如：

``` c
int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 
```

这时，新创建的这个进程将会看到一个全新的进程空间，在这个进程空间里，它的`PID`是`1`。

除了这个 `PID Namespace`，`Linux`还提供了`Mount`、`UTS`、`IPC`、`Network`和`User`这些`Namespace`，其实都是`clone`系统调用的参数，用来对各种不同的进程上下文进行“障眼法”操作。

比如，`Mount Namespace`用于让被隔离进程只看到当前`Namespace`里的挂载点信息；`Network Namespace`用于让被隔离进程看到当前`Namespace`里的网络设备和配置。

## 容器的本质

上面就是容器最基本的实现原理了，`docker`容器听起来玄之又玄的概念，实际上是创建进程时，给这个进程加了一系列需要的`Namespace`参数而已。

通过理解`Namespace`技术，我们已经可以回答一开始的问题了。

**什么是容器？**

**容器的本质是特殊的进程。**

理解这个本质，就应该知道网上很多关于容器的描述并不严谨的，包括博文开头那副图，都是直接把虚拟机的`hypervisor`给替换成一个`docker engine`的模块了。现在我们知道并不存在一个“真正的`docker`容器”运行在宿主机上，一切都是“障眼法”。

容器进程和其他普通进程一样，都由宿主机操作系统统一管理，只不过容器进程拥有额外设置的参数。而`docker`项目更多的是旁路式的辅助和管理工作。所以开头关于容器的图应该如下：

![p2](/img/2018-10-03-p2.png)

## 从Namespace角度看容器优势

到这边我们其实也就明白为什么容器如此受欢迎了。

使用虚拟机技术，必须创建一个虚拟机，这个虚拟机是真实存在的，里面还必须跑着一个真实存在的操作系统，不可避免带来资源损耗，再加上虚拟化软件对宿主操作系统的调用必不可少的拦截和处理，又是一层损耗。

使用容器呢，一个进程而已。不存在虚拟化带来的损耗，也不需要运行单独的操作系统，容器额外占用资源忽略不计。

所以，“敏捷”和“高性能”是容器对虚拟化技术的最大优势。

## 从Namespace角度看容器劣势

有优势就有劣势。

容器的隔离机制其实相比虚拟化技术也有很多不足，最大的不足就是：隔离不彻底。

首先，既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。看似`Mount namespace`很灵活，挂载其他版本操作系统，如`centos`或者`ubuntu`，但并不能改变共享宿主机内核的事实，也就导致了低版本`Linux`宿主机无法运行高版本`Linux`内核的容器。

其次，在 `Linux `内核中，有很多资源和对象是不能被 `Namespace `化的，最典型的例子就是：时间。容器内如果有进程通过系统调用`settimeofday`修改时间，修改的就是整个宿主机的时间，用户需要知道“什么能做，什么不能做”，越狱难度大大降低。

## 关于单进程模型

由于一个容器的本质就是一个进程，用户的应用进程实际上就是容器里`PID=1`的进程，也是其他后续创建的所有进程的父进程。这就意味着，在一个容器中，你没办法同时运行两个不同的应用，除非你的进程是一个公共的`PID=1`的程序，充当两个不同应用的父进程。

但是这么做其实是违背容器设计模式的，一般推荐容器和容器内的应用同生命周期，方便后续容器编排，否则会出现“容器正常，应用已经挂了”的情况，很难处理。

后续如果继续深入学习容器编排方面的知识，保证容器“单进程模型”很重要。

## 关于限制

`Namespace`技术将进程隔离出来后，虽然应用自身认为是`pid=1`的进程，但是在宿主机层面它还是`pid=100`的应用，和其他宿主机是公平竞争关系。也就是说，它可使用的资源（`CPU`，内存）是可以被其他宿主机应用给占用的，也可能它把整个宿主机资源全部吃光。无论哪种行为，这都不是一个合理的“沙盒”应该有的行为。

这个时候就又需要引入`Linux`的`Cgroups`技术了。

## 理解Cgroups

`Cgroups`技术全称`Linux Control Group`。它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 `CPU`、内存、磁盘、网络带宽等等。

`Cgroups`十分简单易用，例如我们想要限制一个进程的资源：

首先跑一个死循环脚本占用系统cpu。

``` shell
[root@vcapp63 cgroup]# while : ; do : ; done &
[1] 12002
```

`top`查看`pid`占用资源情况，发现单核占用`100%`。

``` shell
[root@vcapp63 cgroup]# top -p 12002
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                                                                                                         
12002 root      20   0  116884   1884    136 R 100.0  0.0   1:54.20 bash   
```

`cd`到`/sys/fs/cgroup/cpu`目录下，创建一个文件夹，再`ls`一下，发现里面自动创建了一些文件。

``` shell
[root@vcapp63 cpu]# pwd
/sys/fs/cgroup/cpu
[root@vcapp63 cpu]# mkdir container
[root@vcapp63 cpu]# ls container/
cgroup.clone_children  cgroup.procs  cpuacct.usage         cpu.cfs_period_us  cpu.rt_period_us   cpu.shares  notify_on_release
cgroup.event_control   cpuacct.stat  cpuacct.usage_percpu  cpu.cfs_quota_us   cpu.rt_runtime_us  cpu.stat    tasks
```

这些就是资源限制规则文件集合，关注`cpu.cfs_period_us`和`cpu.cfs_quota_us`两个文件，两个文件要配合使用，一个表示`cpu`的单位时间，另一个表示单位时间内最大分配使用时间，这边默认配置是`100ms`和`-1`，表示不对其做任何不限制。

``` shell
[root@vcapp63 cpu]# cat cpu.cfs_period_us
100000
[root@vcapp63 cpu]# cat cpu.cfs_quota_us 
-1
```

那么我们接下来把刚才的`pid=12002`的死循环进程限制成最多使用`20%`的`cpu`资源，写入进程号和单位时间内最大可分配时间，这里表示每`100ms`的`cpu`时间，该进程最大只能占用`20ms`。

``` shell
[root@vcapp63 cpu]# echo 12002 > /sys/fs/cgroup/cpu/container/tasks
[root@vcapp63 cpu]# echo 20000 > /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us
```

再来查看一下`top`后的`cpu`占用情况，立刻`cpu`占用情况就降低至`20%`了。

``` shell
[root@vcapp63 cpu]# top -p 12002
12002 root      20   0  116884   1884    136 R  20.0  0.0  16:21.47 bash  
```

由此可见，`docker`只要为每一个容器在`cgroups`子系统下，建立一个目录，然后把限制需求和进程`id`写入文件即可完成容器限制能力。

操作下看看是不是这样，再来执行`docker run`，这次带上`cpu`限制参数 。

``` shell
docker run -it --cpu-period=100000 --cpu-quota=20000 172.32.150.63/jsnl/busybox:1.25.0 /bin/sh
```

另开一个窗口，`docker ps –a|grep busybox`一下，找到了容器`id`为`d1a56995a573`的容器。

``` shell
[root@vcapp63 ~]# docker ps -a|grep busybox
d1a56995a573        172.32.150.63/jsnl/busybox:1.25.0             "sh"                     3 minutes ago       Up 3 minutes                                                                               jovial_chaplygin
```

把容器`id`当前缀找到`cgroups`子系统下该容器目录，发现刚才`docker`启动参数的东西已经在这边了。

``` shell
[root@vcapp63 ~]# cat /sys/fs/cgroup/cpu/docker/d1a56995a573…/cpu.cfs_quota_us 
20000
```

很明显`docker`就是通过`Cgroups`技术实现的限制能力。

## 从Cgroup角度看容器劣势

那么`Cgroups`的缺陷也就是`docker`的缺陷，比如限制内存后创建容器`free`和`top`仍然是宿主机的内存。

``` shell
docker run -ti -m 300M 172.32.150.63/jsnl/busybox:1.25.0 /bin/sh
free -g
total       used       free     shared    buffers     cached
Mem:            15          6          8          0          0          4
-/+ buffers/cache:          2         12
Swap:            0          0          0
```

这是因为`Linux`系统`/proc`这个目录记录了系统运行的状态信息，也是`top`和`free`主要的信息来源。而`/proc`这个目录并不知道`Cgroups`给这个容器做了什么样的限制。在生产环境中，这个问题必须进行修正，否则应用程序在容器里读取到的`CPU`核数、可用内存等信息都是宿主机上的数据，这会给应用的运行带来非常大的困惑和风险。(修复关键字`lxcfs`，自己搜索学习，这边不进行展开）

## 继续问问题

`Namespaces`技术`2002`年被嵌入`Linux`系统内核。

`Cgroups`技术年由工程师发起，后很快被嵌入内核。

很明显`Cgroups` 和 `Namespace`两种技术都已经存在多年，而且属于`Linux`无人问津的特性，`docker`项目能在`2014`年无比火爆，频繁被加入业界各大技术会议，甚至引发业界巨头的容器编排战争，它最大的贡献是使用了这两个早已有之的冷门技术么？

`Cgroups`限制容器，`Namespace`隔离容器，它们组成了容器的边界围墙，那容器的地板是什么样子的？换言之，容器里看到的文件系统是什么样子？

## 理解容器镜像

你可能会想到，这一定是关于`mount namespace`的问题，容器里的应用看到的是独立的文件系统，不受宿主机和其他容器影响，真的是这样么？

我们来写个代码使用`mount namespace`来测试下。

``` c
#define _GNU_SOURCE
#include <sys/mount.h>
#include <sys/types.h>
#include <sys/wait.h>
#include <stdio.h>
#include <sched.h>
#include <signal.h>
#include <unistd.h>
#define STACK_SIZE (1024 * 1024)
static char container_stack[STACK_SIZE];
char* const container_args[] = {
  "/bin/bash",
  NULL
};

int container_main(void* arg)
{
  printf("进入容器!\n");
  execv(container_args[0], container_args);
  printf("容器发生了一些错误!\n");
  return 1;
}

int main()
{
  printf("启动一个容器!\n");
  int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWPID | CLONE_NEWNS | SIGCHLD , NULL);
  waitpid(container_pid, NULL, 0);
  printf("结束一个容器!\n");
  return 0;
}
```

这段代码其实非常简单，`main`函数调用`clone`创建一个新的子进程，并声明启用`mount namespace`(`CLONE_NEWNS`标志)，子进程使用`/bin/bash`覆盖自身，也就是说这个应用是个`shell`命令行控制台，这个控制台是启用了在`mount namespace`参数。

编译执行一下。

``` shell
root@vcapp63 docker]# gcc namespaces.c 
[root@vcapp63 docker]# ./a.out 
启动一个容器!
进入容器!
```

我们已经进入了这个启用了`mount namespace`参数的控制台进程了。接着查看`/tmp`目录，发现和宿主机`/tmp`目录完全一样的。

``` shell
[root@vcapp63 docker]# ls /tmp
hsperfdata_root                                                         systemd-private-644e6fb0f83a472a8740e9094d1bafb6-cups.service-BVzhOf
systemd-private-644e6fb0f83a472a8740e9094d1bafb6-bolt.service-0K8NC0    systemd-private-644e6fb0f83a472a8740e9094d1bafb6-rtkit-daemon.service-KltfAD
systemd-private-644e6fb0f83a472a8740e9094d1bafb6-colord.service-sLfA2h  systemd-private-644e6fb0f83a472a8740e9094d1bafb6-spice-vdagentd.service-pmoiWt
```

即使开启了`mount namespace`，容器进程看到的文件系统也是和宿主机完全一致的。

自己安装过`Linux`系统，在`Linux`上挂载过镜像的应该很容易想到，`mount namespace`只是改变了容器挂载点的认知，只有当挂载这个操作，真实发生之后，它才会改变文件系统视图，否则直接继承父进程的文件系统各个挂载点。

那我们就在上面代码子进程`execv`前加一句`mount`挂载操作。

``` c
int container_main(void* arg)
{
  printf("进入容器!\n");
  mount("none", "/tmp", "tmpfs", 0, "");  
  execv(container_args[0], container_args);
  printf("容器发生了一些错误!\n");
  return 1;
}
```

然后重新编译，运行，`ls`，发现容器进程内的`/tmp`目录变成一个空目录了。

``` shell
[root@vcapp63 docker]# ./a.out 
启动一个容器!
进入容器!
[root@vcapp63 docker]# ls /tmp/
```

这边我们启用了`mount namespace`参数，然后进行`mount`操作，要注意这个挂载操作是只在容器进程内进行，宿主机的挂载是不受影响的。

这也是`mount namespace`和其他`namespace`不同的地方，`mount namespace`对进程视图的改变，一定要和`mount`操作一起才会生效。

那我作为用户，我觉得这么很烦，我希望每次创建容器，都已经是独立视图了，不需要一个个重新挂载，这可以实现么？

也不难想到，`docker`启动一个容器，会直接重新挂载它的整个根目录`/`即可实现。为了这个环境看着比较真实，我们一般在容器的根目录下挂载整个操作系统的文件，比如我们可以挂载一个`ubuntu`操作系统的所有目录和文件。

这个挂载在容器根目录上，用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：`rootfs`（根文件系统）。

至此一个`docker`完整的容器在你面前已经没有秘密可言了。

它的核心原理是一个进程，它在待创建时：

1. 启用`linux namespace`配置。

2. 设置指定的`cgroups`参数。

3. 切换进程根目录，也就是挂载新的`rootfs`（容器镜像）。

## 从rootfs角度看容器劣势

既然谈到`rootfs`，就要明确，`rootfs`是一个操作系统的文件、配置和目录，并不包括操作系统内核。在`Linux`中，这两部分是分开存放的，操作系统只有在开机时才会加载指定版本操作系统内核。`Rootfs`只是操作系统躯壳，没有灵魂。

灵魂在哪儿？

上面提到过，所有容器共享操作系统内核，所以容器如果需要配置内核参数，加载额外模块，一定要慎重。因为你操作的都是宿主机内核，所有容器的“全局变量”。

这也是容器相对虚拟机来说最大的劣势，毕竟虚拟机不仅模拟硬件机器，每个虚拟机上跑的沙盒还会运行独立操作系统。

## 从rootfs角度看容器优势

不过，也正是因为容器这种`rootfs`的实现，才有了容器最大的优势：“一致性”。

什么是“容器一致性”？

大家开发其实经常会遇到一些本地与云端不一致的环境问题。比如“我本地是好的啊？”，“服务器上缺数据库配置”，“服务器上某某某路径没有建”。。。

有了`rootfs`之后，这些问题被非常优雅的解决了，`rootfs`打包的不只是应用，还有整个操作系统的文件和目录，也就是说应用和它的所有依赖都被打包在一起了。

大多数开发者对依赖的理解常常局限在语言和配置层面，比如`jdk`，`xml`配置等，忽视了整个操作系统才是它需要的完整依赖。

`docker`这种深入到操作系统级别的运行环境一致性，打通了应用在本地开发和远端执行环境之间难以逾越的鸿沟。

## Docker项目最大的创新，也是其制胜法宝

然后你是不是会疑问，难道我每次写了一个新的应用都要重复制作一个`rootfs`么？

比如我用`centos`做了一个`rootfs`，然后又在里面安装了`jdk`环境，部署了自己的`java`应用。之后我同事发布自己的`java`应用时，显然是希望直接使用我做的`jdk`环境的`rootfs`。

一种方式就是，我每做一步，我就打包一个`rootfs`出来，然后同事`A`就可以找到我刚安装好`jdk`环境的那个`rootfs`来发布自己的应用。但是这种方式，如果同事`A`修改了我的这个`rootfs`，他的`rootfs`和我原来的`rootfs`直接就没有关系了，大家都是独立的`rootfs`，极度碎片化。

这个时候就要谈到`docker`公司对容器技术的最大创新，定义了一种镜像打包通用标准。`docker`在镜像的设计中，引入了层的概念。也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 `rootfs`。

当然，这个增量`rootfs`的设计也不是`docker`公司凭空想出来的，这种技术又是早已有之，名为联合文件系统（`Union File System`） 。

## 理解联合文件系统

联合文件系统最主要的功能是把不同位置的目录挂载到同一个目录下。比如，我现在有两个目录`A`和`B`，他们的目录结构如下：

``` shell
qiubinren@ubuntu:~/aufs$ tree
.
├── A
│   ├── a
│   └── x
└── B
    ├── b
    └── x
```

然后使用联合挂载的方式，挂载到一个公共目录`C`上。

``` shell
qiubinren@ubuntu:~/aufs$ mkdir C
qiubinren@ubuntu:~/aufs$ sudo mount -t aufs -o dirs=./A:./B none ./C
```

查看目录`C`的内容，发现`A`和`B`内容已经被合并在一起了。

``` shell
qiubinren@ubuntu:~/aufs$ tree ./C
./C
├── a
├── b
└── x
```

`C`的内容既包含`A`的，也包含`B`的，而且`x`文件只有一份，这就是联合文件系统的合并操作。修改`C`中的这些文件，`A`和`B`中的文件也会被修改。

这边为方便演示，使用的是最简单最容易理解的`aufs`，`aufs`并没有被加入`Linux`内核，只能在`ubuntu`上使用，`centos`上要支持需要加载内核模块，所以`docker`支持多种类型的存储支持，在`centos`上的实现其实还不太一样。

## 理解Docker的rootfs

接下来我们使用`docker pull ubuntu`命令到`docker hub`上拉取一个镜像。这个所谓的“镜像”其实就是一个`ubuntu`操作系统的`rootfs`。与之前讲的`rootfs`不同的地方是，`docker`镜像的`rootfs`往往由多个“层”组成的。

这边查看`ubuntu`镜像详细信息。忽略大部分信息，只看`rootfs`部分。

``` shell
docker image inspect ubuntu:latest
...
"RootFS": {
            "Type": "layers",
            "Layers": [
                "sha256:bebe7ce6215aee349bee5d67222abeb5c5a834bbeaa2f2f5d05363d9fd68db41",
                "sha256:283fb404ea9415ab48456fd8a82b153b1a719491cdf7b806d1853b047d00f27f",
                "sha256:663e8522d78b5b767f15b2e43885da5975068e3195bbbfa8fc3a082297a361c1",
                "sha256:4b7d93055d8781d27259ba5780938e6a78d8ef691c94ee9abc3616c1b009ec4a"
            ]
        },
...
```

可以看到，这个`ubuntu`镜像实际上由`4`个层组成。这`4`个层其实就是`4`个增量`rootfs`，每一层都是`ubuntu`操作系统文件与目录的一个部分。`Docker`在运行容器，使用镜像的时候，就是把这`4`个层统一挂载到一个统一挂载点上（上面那个例子中的`./C`目录）。

我们使用这个`ubuntu`镜像，启动一个容器来看看。`-d`参数使用`ubuntu`镜像后台启动一个容器，容器的任务是`sleep`一小时。

``` shell
docker run -d ubuntu:latest sleep 3600
```

我们到`docker`默认挂载点去`ls`一下，不出意外，目录下是个完整的`ubuntu`操作系统。

``` shell
# ls /var/lib/docker/aufs/mnt/36e03a80bceefa23ca34862d577aacde560bdcb6d5279222a68ca8c67012259e
bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
```

那么前面提到的`4`层怎么联合挂载成这样一个完整的`ubuntu`操作系统文件和目录的呢？

首先，`aufs`的挂载信息都记录在系统目录`/sys/fs/aufs`下面。

``` shell
cat /proc/mounts |grep 36e03a80bceefa23ca34862d577aacde560bdcb6d5279222a68ca8c67012259e

none /var/lib/docker/aufs/mnt/36e03a80bceefa23ca34862d577aacde560bdcb6d5279222a68ca8c67012259e aufs rw,relatime,si=f93b93d82a982440,dio,dirperm1 0 0
```

我们找到了这个`aufs`的内部`id`，也叫`si`。使用这个`id`，你就可以在`/sys/fs/aufs`下查看各层挂载信息。

``` shell
cat /sys/fs/aufs/si_f93b93d82a982440/br[0-9]*

/var/lib/docker/aufs/diff/36e03a80bceefa23ca34862d577aacde560bdcb6d5279222a68ca8c67012259e=rw
/var/lib/docker/aufs/diff/36e03a80bceefa23ca34862d577aacde560bdcb6d5279222a68ca8c67012259e-init=ro+wh
/var/lib/docker/aufs/diff/6b662604b504539bb0e090d1c0c3ad36410ebab6098df92af21f878e4af80875=ro+wh
/var/lib/docker/aufs/diff/2068c22f7699b5cd4dc70a690196ac04d7e642c6ee03f26d8170e9195edfc726=ro+wh
/var/lib/docker/aufs/diff/1ccbfdfe3d16ccb6cf1a66d42813cb1a6f2cb527e437d3b4a685b5cfddb97c52=ro+wh
/var/lib/docker/aufs/diff/3adb0cab653e75cc76dd8c5719aa9b58ea197a54d82b6968498ba11d2c464f68=ro+wh
```

这些信息里，我们看到容器的层，都是放在`/var/lib/docker/aufs/diff/`下，然后启动容器后被联合挂载到`/var/lib/docker/aufs/mnt/`下。而且刚才镜像是`4`个层，容器却有`6`个层，多了两个层，`4`个镜像层是只读的，多出一个只读层，还有一个可读可写层，这两层是什么东西？

根据返回结果每行末尾的信息，我们明显可以看出容器的`rootfs`由三个部分组成。我们用图画出来，如下所示：

![p3](/img/2018-10-03-p3.png)

第一部分：只读层。

它是容器`rootfs`最下面的`4`层，对应的正是`ubuntu`镜像的`4`层。可以看到它们的挂载方式都是只读的（`readonly writeout`）。我们可以分别看下它们的内容，可以发现每个目录内（也就是每一层）都是`ubuntu`操作系统的一部分。

第二部分： 可读写层。

它是容器`rootfs`最上面的一层，挂载方式是可读可写。容器启动后，没有写入操作的时候，它就是空的，一旦有写入操作，就会增量写到这一层里。

可是，如果我想在容器里删除一个镜像自带的文件，怎么办？这个文件是只读层里的，我的所有写操作都在可读写层。

这就要谈到`writeout`了。`aufs`会在可读写层创建一个隐藏的`writeout`文件，把只读层文件“遮挡”起来。这个功能就是上面只读层，`readonly`后面的`writeout`的含义了。

所以最上面的可读可写层就是用来存放你对`rootfs`产生的增量修改，无论增删改都发生这里。

修改之后，你可以使用`docker commit`命令，保存这个被修改的可读可写层，原先的只读层不会有任何修改。这就是增量`rootfs`的好处。

第三部分： init层。

同事有一次制作镜像的时候遇到了一个问题，然后跑过来问我，为什么他制作镜像的时候，明明修改了`/etc/hosts和/etc/resolv.conf`里的内容，加了一些域名和`ip`的对应关系，`DNS`信息，执行`docker
commit`之后，把容器打包成镜像，启动一个容器，这些信息居然又没了？怀疑他的`docker`版本存在`bug`。

这边的秘密其实就是`docker`容器的`init`层。`init`层是`docker`项目单独生成的一个内部层，专门用来存放`/etc/hosts`，`/etc/resolv.conf`等信息。

为什么需要这一层呢？

这些文件本来属于只读层，但是用户往往在启动之后，需要在里面写入一些指定的值，比如`hostname`，除非你这个镜像只在一台主机上跑，毫无通用性，不然必然会修改这些信息。但是这些修改呢，往往又只对当前容器生效。换台主机换个容器实例可能又没用了，更甚至反而导致一些问题，这个时候我又不得不再次修改，所以我就不希望这些内容，`docker commit`的时候，它会被提交进只读层。

`docker`的做法，这些文件单独以一个层挂载出来，`docker commit`的时候，只会提交可读可写层的修改内容，不包括这些文件。

至此，`docker`的`rootfs`也已经全部讲解完毕，最终，这`6`层都会被挂载到一个目录下，被当成一个完整的`ubuntu`操作系统被容器使用。

最后我们在来总结一下`rootfs`，它只是一个操作系统的所有文件和目录，并不包含内核，最多也就几百`M`，几个`G`。而相比之下，传统虚拟机的镜像大多是一个磁盘的“快照”，磁盘有多大，镜像就能有多大。

`Docker`通过“分层设计”，增量式`rootfs`让不同公司，不同开发人员紧密联合在一次，每一次镜像的推送和拉取，比原来整个操作系统要小很多，只读层不变特性形成了共享层，也会让所有镜像总占用空间小的多。使得“容器一致性”环境变为可能，打通了“开发-测试-部署”的环境限制，成为可见未来的主流发布模式。

## docker实战与扩展

接下来，我们就用个`helloworld`程序实战下`docker`发布流程。

首先我们仿照我司`dubbo`服务结构，写一个极度简单的`python`服务。目录结构如下：

``` shell
qiubinrendeMacBook-Pro:shizhan qiubinren$ tree accting/
accting/
├── Dockerfile
├── biz_basic.py
├── requirements.txt
└── server_accting.py
```

先来看看这个`python`服务是干啥的。

`biz_basic.py`文件很简单，就是定义了个`hello`函数，直接返回`hello world`。

``` python
qiubinrendeMacBook-Pro:accting qiubinren$ cat biz_basic.py 
#!/usr/bin/python
# -*- coding: UTF-8 -*-
def hello():
    return "hello world”
```

`requirements.txt`里是这个`python`应用的依赖，我们需要依赖`Flask`框架。

``` shell
qiubinrendeMacBook-Pro:accting qiubinren$ cat requirements.txt 
Flask
```

`server_accting.py`是服务源码文件：

``` python
qiubinrendeMacBook-Pro:accting qiubinren$ cat server_accting.py 
#!/usr/bin/python
# -*- coding: UTF-8 -*-
from flask import Flask
import socket
import biz_basic

app = Flask(__name__)

@app.route('/')
def accting():
    html = "<h3>{tmp} 我是 {name}!</h3>”
    return html.format(tmp=biz_basic.hello(), name="acting”)
    
if __name__ == "__main__":
    app.run(host='0.0.0.0', port=80)
```

`server_acting.py`就是返回个简单的网页，里面会调用`biz_basic`里的`hello`函数获取字符串`hello
world`。这个应用运行效果如下。

浏览器访问地址`http://127.0.0.1:80`，显示页面。

![p4](/img/2018-10-03-p4.png)

那么现在我们要把这个`python`服务容器化。

容器化第一步，制作镜像。因为我们是制作`python`服务镜像，先找个安装过`python`环境的基础镜像包。这边我选择官网`python:2.7-slim`这个基础镜像。

执行`docker images`查看本地已经有的镜像，并没有这个镜像。

![p5](/img/2018-10-03-p5.png)

执行下面命令，拉取基础镜像。

``` shell
docker pull python:2.7-slim
```

![p6](/img/2018-10-03-p6.png)

再执行一下`docker images`命令发现本地现在已经有了这个镜像。

![p7](/img/2018-10-03-p7.png)

这边介绍两种构建镜像方式，第一种一般不用，不过这边为了让不熟悉`docker`的了解一些`docker`命令，所以也介绍一下。

先把刚才下的`python`镜像启动起来。

``` shell
docker run –ti python:2.7-slim /bin/bash
```

![p8](/img/2018-10-03-p8.png)

重新在宿主机上开一个控制台，`docker ps –a`一下，发现我们现在已经启动了一个容器。

![p9](/img/2018-10-03-p9.png)

在容器内新建一个`app`目录。

![p10](/img/2018-10-03-p10.png)

在宿主机上执行`docker cp`命令，把宿主机上必要的文件都拷贝到容器内的`app`目录下。

![p11](/img/2018-10-03-p11.png)

容器内安装`python`的`flask`框架。

![p12](/img/2018-10-03-p12.png)

使用`docker commit`命令将这个容器构建成镜像。

![p13](/img/2018-10-03-p13.png)

构建镜像完成了，那么第二步，使用镜像启动一个容器。我们执行`docker run –d –p 4000:80 server_acting:1.0 python /app/server_acting.py`命令。

![p14](/img/2018-10-03-p14.png)

这条命令中`docker run`就不解释了，启动一个容器，`-d`是后台启动，`-p`是宿主机端口`4000`映射成容器内`80`端口，`server_acting:1.0`是镜像名和版本号，执行`python /app/server_acting.py`命令。`docker ps –a`看到`status`是`up`状态，说明是启动成功的。

浏览器访问本机`4000`端口，服务一切正常。

![p15](/img/2018-10-03-p15.png)

这就是第一种方式，不太常用，因为你如果修改一下`biz_basic.py`，然后又一次构建同样的镜像，你又得重复这么操作一遍，这边只是为了让大家尽可能了解多的命令，才这么构建。

下面看看，常用的构建镜像方式，也是`docker`提供的便捷方式，`Dockerfile`方式。首先我们把刚才构建的镜像和容器都删掉。

停止容器。

``` shell
docker stop 5e09fbb5b933
```

删除容器。

``` shell
docker rmi server_acting:1.0
```

然后看看Dockerfile怎么写的。

``` dockerfile
# 使用官方提供的Python开发镜像作为基础镜像
FROM python:2.7-slim

# 将工作目录切换为/app
WORKDIR /app

# 将当前目录下的所有内容复制到/app下
ADD . /app

# 使用pip命令安装这个应用所需要的依赖
RUN pip install --trusted-host pypi.python.org -r requirements.txt

# 允许外界访问容器的80端口
EXPOSE 80

# 设置容器进程为： python server_accting.py，即：这个Python应用的启动命令
CMD ["python", "server_accting.py"]
```

通过这个文件，你可以看到`Dockerfile`的设计思想，是使用一些标准的原语（即文件中大写词语），描述我们所要构建的`Docker`镜像。并且这些原语，都是按顺序处理的。

`FROM`原语，指定基础镜像，免去了安装`python`的工作，否则，如果你的基础镜像是`ubuntu`操作系统的镜像，你需要先加一步安装`python`环境。

``` dockerfile
FROM ubuntu:latest
RUN apt-get update –y
RUN apt-get install -y python-pip python-dev build-essential...
```

`RUN`原语，就是在容器里执行`shell`命令的意思。

`WORKDIR`原语，设置工作目录，意思是在这一句之后，`Dockerfile` 后面的操作都以这一句指定的`/app`目录作为当前目录。

最后`CMD`原语，指定容器默认启动应用，等价于下面这条启动容器命令：

``` shell
docker run server_acting:1.0 python /app/server_acting.py
```

一个`Dockerfile`只能有一个`CMD`原语，如果有多个，只有最后一个生效。

好了，现在我们写完了`Dockerfile`了，接下可以使用`docker
build`命令构建镜像了。

``` shell
docker build –t server_acting:1.0 .
```

其中`-t`是给镜像加个标签，起个名字。

`docker build`会自动加载当前目录下的`Dockerfile`文件，然后按照顺序，执行文件中的原语。

需要注意的是，`Dockerfile`中的每个原语执行后，都会生成一个对应的镜像层。即使原语本身并没有明显地修改文件的操作（比如`WORKDIR`原语），它也会生成一个对应的层。只不过在外界看来，这个层是空的。

所以如果有一些可以合成一个原语的操作我们尽量在`Dockerfile`中合成一个语句，这也是看很多网上`Dockerfile`例子的时候，经常会出现的情况。比如：

``` shell
RUN go build -a -o /go/bin/harbor_jobservice \
    && chmod u+x /go/bin/harbor_jobservice
```

将多条`shell`命令使用`&&`连接后合并到一个`RUN`原语中。

生成完镜像，就可以启动这个新的镜像，因为我们在`Dockerfile`中指定了`CMD`，所以这边可以不写容器指令。

``` shell
docker run -d -p 4000:80 server_acting:1.0
```

打开浏览器页面，查看结果。

![p16](/img/2018-10-03-p16.png)

启动成功。

现在我们想要让这个镜像可以给其他人使用。 

我们要把这个容器镜像推送到私有仓库上。这边使用我搭建的`172.32.150.63`的`harbor`仓库。

我先使用`docker tag`给这个镜像一个正式的名字。

``` shell
docker tag server_acting:1.0 172.32.150.63/jsnl/server_acting:1.0
```

然后我们登录我的私有仓库。

``` shell
docker login 172.32.150.63
```

输入用户名和密码。

``` shell
docker push 172.32.150.63/jsnl/server_acting:1.0
```

这样我就把镜像上传到我的私库去了，其他人要使用只要使用`docker pull`命令即可拉取我的镜像。

``` shell
docker pull 172.32.150.63/jsnl/server_acting:1.0
```

我们还可以使用`docker exec`命令进入一个正在运行的容器，修改一小部分文件立刻提交成下一个版本的镜像。

![p17](/img/2018-10-03-p17.png)

这边我们进入了一个正在运行的容器，新建了一个`test.txt`然后退出再使用`commit`命令生成`server_acting:2.0`镜像。

![p18](/img/2018-10-03-p18.png)

还记得上面我们说过的`namespace`隔离机制么？容器只是一个启用`namespace`机制的进程，那是不是很奇怪，`docker exec`是怎么“进入”这个进程里的呢？

实际上，`namespace`好像看不见摸不着只是设置的参数，但一个进程的`namespace`信息在`linux`上是以文件形式存储的。

我们在宿主机上使用`docker inspect`命令即可查看容器详细信息。输入`docker inspect 容器id`命令，省略大部分信息，找到宿主机上，这个容器真正的`pid`，这边显示为`8070`。

``` shell
"State": {
            "Status": "running",
            "Running": true,
            "Paused": false,
            "Restarting": false,
            "OOMKilled": false,
            "Dead": false,
            "Pid": 8070,
            "ExitCode": 0,
            "Error": "",
            "StartedAt": "2019-02-21T06:16:08.383283496Z",
            "FinishedAt": "0001-01-01T00:00:00Z"
        },
```

查看`/proc/8070/ns`，就是这个容器对应的`namespace`文件。

![p19](/img/2018-10-03-p19.png)

可以看到一个进程下的都是虚拟文件，通过链接到真实`namespace`文件上。

那`docker`怎么实现的“加入容器”的？

其实是`docker`真正的操作是，新建一个进程，获取想加入的进程的`namespace`文件操作符，然后通过`Linux`自带的`setns()`系统调用实现（这边简单介绍，感兴趣自己实现一个代码试试）。

这种实现方式意味着，一个进程可以选择加入到某个进程已有的`Namespace`当中，从而达到“进入”这个进程所在容器的目的，这正是`docker exec`的实现原理。

转了一大圈，我们发现我们对`docker exec`的实现的研究，其实是对上面介绍的`Linux Namespace`技术实现的补充。

这种通过操作系统进程相关的知识，逐步剖析`Docker`容器的方法，是理解容器以及遇到容器相关问题时候解决的一个关键思路。

最后一个问题。

上面那个`server_accting`架构，其实和我司的`dubbo`服务业务层架构很相似。

`flask`框架可以看成公司服务层使用的`dubbo`框架。

`server_acting.py`可以看成公司的`server_acting.jar`。

`biz_basic.py`可以看成公司的`biz_basic.jar`。

我们现在这种打包方式，一个服务应用所有`jar`全部打成一个镜像。这种方式真的好么？有什么问题？

现在我们只有一个`server_acting`服务，后面我们新增`server_billing`，`server_info`，`server_sps`等，一大堆服务。

那么现在有问题了，看下以下两个场景。

场景一：

所有这些服务全部都依赖`biz_basic`。如果我现在`biz_basic`需要修改一下，不想返回`hello
world`了，想返回中文“你好！世界！”，那我要怎么做？

我是不是要先修改代码，重新编译，然后重新构建`server_acting`，`server_billing`，`server_info`，`server_sps`一大堆镜像？

场景二：

这次上线`biz_basic`和`server_accting`都有改造，上线后发现`biz_basic`改的有问题要回退，但是`server_accting`的东西，我们不想回退，怎么办？

这种构建模式，是不是只能先把`biz_basic`代码先回退，再用回退后的`biz_basic`和`server_accting`重新构建一次镜像了。

从这两个场景来看，这样的打包方式很明显对上线和回退不太友好，每次修改的构建量都非常大。它们不是业界最佳实践。

## 容器编排基础

只用容器技术，我们似乎只有两种构建镜像的方式。

一种呢，就是上面这种我司采用的方式，这种方式的问题说了，代码修改后，构建量大，回退也不方便。

另一种呢，也可以比较容易想到，使用`docker`的`volume`挂载，镜像只打框架包，然后业务包，通过挂载宿主机目录的方式。容器启动时，`docker run`加入`-v`参数挂载宿主机业务包目录，使容器内的应用可以读取到业务包。这么做也有问题，我们必须要解决一个问题，怎么保证每台主机启动容器前，磁盘上已经存在这个应用业务包的目录呢？这么做是不是要另外独立维护一套分布式存储系统，所有主机使用这套分布式文件系统，增加系统复杂度和维护成本。

业界怎么解决这个问题的？最佳实践是什么样的？

这边需要介绍个最常用的“容器设计模式” 之`sidecar`。

思想其实有点类似上面介绍的“加入”`namespace`的能力。

首先，我们把`dubbo`框架以及一些其他框架包打成一个镜像，框架包变更次数远远低于业务包，这些`jar`包打成一个镜像，这个镜像版本不会太多，变更也不会太多。

然后我们把`server_accting`单独打成一个镜像。

把`biz_basic`单独打成一个镜像。

这样其实每一次变更，每一个版本的`jar`包就对应一个内部只包含了这个`jar`的镜像版本。

然后在启动`server_accting`服务之前，这些辅助镜像依次启动，每个容器仅仅做一件事，挂载目录，将镜像内的业务`jar`包`cp`到挂载目录下，之后启动`dubbo`框架包镜像挂载此目录。

这样每个应用其实变成了，镜像之间的“组合”关系，你可以定义，需要哪些版本的哪些镜像，而不必频繁打包应用镜像。每次变更业务包，只需重新单独构建它自己的那个镜像即可。镜像大小也大大减少，因为这些辅助镜像的任务只有一个单纯`cp`，所以基础镜像你甚至不用采用完整的操作系统目录，可以构建一个极其精简，只有`cp`命令的镜像来完成这个工作。

这个东西其实是现在非常火的容器编排工具`kubernetes`（`k8s`）的一个小功能，叫“初始化容器”。这种镜像之间的“组合”关系，会定义在一份`kubernetes`的编排文件中。开发提交一个应用相关信息给维护的时候，其实是提交多个镜像以及这份组合关系文件。

这就是业界面对这类问题的最佳实践。

由于涉及容器编排知识，这边只是简单描述一下。