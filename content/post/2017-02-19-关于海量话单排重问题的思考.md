---
title: 关于海量话单排重问题的思考
date: 2017-02-19 13:32:26
tags: ["Bigdata", "Bloom Filter", "Hbase"]
categories: ["Bigdata", "Bloom Filter", "Hbase"]
---
# 关于海量话单排重问题的思考

## 问题描述

该问题的起因和描述都在两周前的这篇博文里[大数据话单排重任务
](http://www.qiubinren.com/blog/2017/02/%E5%A4%A7%E6%95%B0%E6%8D%AE%E8%AF%9D%E5%8D%95%E6%8E%92%E9%87%8D%E4%BB%BB%E5%8A%A1.html)。  
这边再大概说下问题：每天60亿话单，里面有重单，现在要排重。  
> 1.话单时间范围可能会出现上个月的话单，这类话单也要排重，所以至少存两个月的话单数据。  
2.大部分话单其实都是当天的。

跟两周前发的博客有区别，这次确认了上面两点需求，首先每天的话单量大概确认了，差不多是60亿每天，其次话单时间也确认了，大部分为当天，少量其他时间的数据。

<!--more-->
## 使用内存数据库的解决思路
跟两周前博客写的一样，现在这两点需求一确认，热点数据很明显了，这一两天的话单就是热点数据，其他时间的话单只是很少一部分。那么方案就是这样的：  

1. 把这一两天的话单按天在分布式内存数据库里建`cache`，`cache`只存`key`，其他时间的话单`key`都在`HBase`中。
2. 有话单来，先判时间，是这一两天的，直接在对应时间的cache里做排重，没有重复的话，把`key`插入内存数据库，并且同步一份到`HBase`（`HBase`的`LSM`树写很快，随机读慢，大量写的场景很适合它）。
3. 两天以前的话单，这是很小的数据量，直接甩给另一个进程或者线程到`HBase`里做排重，这边是随机读的场景，可能会慢，但是`HBase`毕竟资料丰富，网上也有很多高效随机读的解决方案，少量数据随机读是属于可以解决的问题。
4. 过期数据清理，因为`cache`的数据`HBase`里也有，所以换天的时候，可以直接建新`cache`，把两天前的`cache`直接删了，不用考虑同步，HBase的数据清理，就换月把两个月前的`key`给清理了。  

### 方案缺陷
1. 占大量内存空间，一天60亿话单的`key`，两天就是120亿，按每个`key`占100字节来算，加起来就是占12000亿字节，大概就是1118GB的内存，这个纯内存数据集群不小啦，如果没有这么多内存，那就退而求其次，保证一天的数据量，如果连存一天数据量的内存都没有，`cache`就要用持久化的缓存集群来做了，那又要开始考虑内存和磁盘数据淘汰机制了，这个场景一天之内的数据存在不存在热点数据并不好判断，大量读命中不了`cache`的话，缓存数据库反复内存和磁盘置换数据，磁盘`io`性能消耗太大，效率会不会太低了。
2. 这么多内存存的话单`key`只为了这个需求服务，这么一大片的内存没有其他读写需求，是否值得呢？

**那么有没有办法，时间稍微慢点，cache空间大大减小的方案呢**

**在这个场景下，这个方法其实是存在的，这边先介绍个算法，然后再分析**

## 使用布隆过滤器的解决思路
### 算法介绍
`HBase`这类数据库，为了增加随机读的效率，可以选择开启一个叫布隆过滤器(`bloom filter`)的算法。算法的详细介绍感兴趣可以参考[布隆过滤器详解](http://blog.csdn.net/lifuxiangcaohui/article/details/42025629)。这边就简要说下算法的思路：  

> 内存里开一个二进制全零阵列，把key通过k个hash函数，映射成阵列上的m个点。如果这m个点有一个点为0，则这个key没出现过，或者说不在前面的key构建的那个集合里，如果全1，则这个key，“可能”已经在这个集合里了。

算法说`key`没出现过，那它肯定没出现过，说它出现过，则可能没出现过，只是概率比较小。不会漏判，但会误判，存在误判率，所以不适合对错误零容忍的场景。`HBase`用它来判断要读的`key`在不在这个`region`上。**(这边可能有人会奇怪，我们的场景不是零容忍场景么？漏话单这种事情肯定是不可接受的呀！！别急，看下去)**

#### 算法优势
1. 空间大幅度降低，每个数据都用几个二进制位来映射，只需维护这张位图就行了。
2. 判重时间复杂度`O(K)`，跟数据量关系不大，和`hash`函数个数有关，基本上可以认为是常数级的时间复杂度了，速度非常快。

#### 算法缺陷
1. 存在误判率
2. 不支持删除集合中某数据的映射

误判率`p`和二进制阵列位数`m`以及数据量`n`的关系是，二进制阵列位数`m`越大，`p`约小，数据量`n`越大，`p`越大，具体公式和推导可以看上面的链接。  
这边根据公式可以根据数据量`n`和能容忍的误判率`p`得出**最少**需要的`hash`函数个数，和内存容量。我把公式写成了`python`脚本。可以直接跑一下算得。

``` python
#!/usr/bin/env python3
import cmath

#输入估计的数据量
elementNum = int(input("please input n:"))

#输入可以接受的误判率
error_rate = float(input("please input error_rate:"))

bit_num = -1 * elementNum * cmath.log(error_rate) / (cmath.log(2.0) * cmath.log(2.0))

hash_num = cmath.log(2) * bit_num / elementNum

#输出要的二进制位个数
print("bit_num=", bit_num)

#输出需要的hash函数个数
print("hash_num=", hash_num)

#输出那么多二进制位数占的内存大小
print("need GB = ", bit_num / 8 / 1024 / 1024 / 1024)
```

#### 算法测试
**算法的具体实现，google已经给我们实现了，网上找个包就行了。  
找包的同时发现了这个包的一个坑[google guava bloom filter包的坑](http://blog.csdn.net/inte_sleeper/article/details/11687707)，这个坑会对我们应用有影响么，思考后发现并没有，反而可能会有帮助，还是后面说。**

这边我们先测试下，一亿的`key`，用布隆过滤器排重效率怎么样。  
首先看下一亿个`key`，控制误判率`p=0.00001`，大概需要多少内存，跑下程序：

``` shell
./bloomfilter.py 
please input n:100000000
please input error_rate:0.00001
bit_num= (2396264594.34186-0j)
hash_num= (16.609640474436812+0j)
need GB =  (0.278961913932797-0j)
```

一亿`Key`判重，只要`300M`内存。
开始吧，首先随机生成一亿个不重复`key`，这边预先用`JAVA`跑出一亿个`UUID`存在文件中，然后，使用布隆过滤器判重为重复的数据就认为是误判。测试代码如下:

``` java
import com.google.common.base.Charsets;
import com.google.common.hash.BloomFilter;
import com.google.common.hash.Funnels;

import java.io.*;

/**
 * Created by qiubinren on 17/2/18.
 */


public class BloomFilterDemo {
    private static BloomFilter<CharSequence> filter = BloomFilter.create(Funnels.stringFunnel(Charsets.UTF_8), 100000000, 0.00001F);

    public static void main(String args[]) throws IOException {

        FileReader reader = new FileReader("//Users//qiubinren//study//python//key.txt");
        BufferedReader br = new BufferedReader(reader);
        int count=0;
        String str = null;
        long startTime = System.currentTimeMillis();
        while((str = br.readLine()) != null) {
            boolean exists = filter.mightContain(str);
            if(exists)
                count++;
            else
                filter.put(str);
        }
        long endTime = System.currentTimeMillis();
        br.close();
        FileWriter writer = new FileWriter("//Users//qiubinren//study//python//time.txt");
        BufferedWriter bw = new BufferedWriter(writer);
        bw.write("1亿条数据排重时间： "+(endTime-startTime)/1000+"s,误判数:"+count+",误判率:"+count/600000000.0);
        bw.write('\n');
        bw.close();

    }

    private synchronized boolean containsDealId(String deal_id){

        if(deal_id == null){
            return true;
        }

        boolean exists = filter.mightContain(deal_id);
        if(!exists){
            filter.put(deal_id);
        }
        return exists;
    }


}
```

很快就跑完了，我们看下程序打印的性能报告和误判个数：

```
cat time.txt 
1亿条数据排重时间： 155s,误判数:78
```
**一亿数据排重只用了155s，误判个数只有78。**

**这个时候肯定有人要问，这个算法在这个场景下怎么用呢？这个时候我们回头看这个项目场景，重单数量对于整个60亿数据来说，属于非常少量的数据。**

**这个场景使用布隆过滤器，花很少的时间就可以把60亿数据中大量的确定不重复数据的key过滤出来(分出60个通道，保证每个通道的数据量在一亿左右，上面的链接也看了，google的包有坑，超过两亿hash函数就选取不太好，容易退化，误判率升高。不过无所谓，分通道并发是必须的。)**


### 布隆过滤器方案

**那么使用布隆过滤器后，到HBase里随机读的操作有哪些数据？很明显现在需要到HBase里读的数据有：非这一两天的数据（少）+这一两天真正重复的数据（少）+误判数据（可以控制的很少，60个通道加起来也就在6万数据上下）**

1. 首先最上游有个分发线程，不断读话单文件，手机号取模（或者其他规则），扔到不同的通道里去。
2. 每个通道有两个布隆过滤器，一个昨天的，一个今天的。取到话单，先判时间，时间是这两天的，进行判重，不是的直接扔给下游确认进程。
3. 被判断为重单的话单也扔给下游确认进程（这里面含确实重复和误判，其中误判很小），判断为非重单的话单，直接写入`HBase`。
4. 确认进程进行判重写入，这边随机读速度慢，最好想办法优化，是不是可以`key`攒起来排序之类的操作，多条一起查呢。
5. 换天的时候，把两天前的布隆过滤器给清空。
6. 若遇到宕机，今天的布隆过滤器重新搭建起来很简单，把已判数据在跑一边，分分钟的事情。

**上面很多问题得到了解答，重单属于少数话单，那么时间的复杂程度增加不大的。另外算法另一个缺陷不支持删除数据的缺陷，我们这个场景不需要，要删也是按天全部清空。**

#### 方案优势
保证速度的前提下，异常省空间，就看重复话单的数量，`HBase`随机读那边的性能可以接受多少了。

#### 方案缺陷
很明显，要是重复话单多，大量重单压在`HBase`那边二次排重性能会比较差。

## 总结
**重单数量少这个很关键，如果重单数量大到一定程度，那么这个省空间的方案性能会比较差了。**

**再说下，HBase既然支持布隆过滤器，为啥选择把它给提出来。首先HBase的布隆过滤器不一定在缓存里，60亿次读HBase的话，每次没命中，缓存磁盘置换布隆过滤器要一定时间。其次HBase的布隆过滤器只是判断key在不在这个region，而我们这边是明确知道规则，控制数量的直接布隆过滤，这个效率差距肯定不是一点半点，速度远远要快。**

**其实呢，方案思路总结起来很简单，就是拿布隆过滤器把大部分数据给过滤出来，变成直接写入，让只有少部分的数据不得不读了再写。写入HBase的目的，其实也是为这少部分数据服务的，有点像二八定理，百分之八十的数据直接就出结果了，百分之二十的数据却占了大量的资源和时间。**

**其实下游这个数据库不一定必须HBase。只要满足大数据下写速度飞快，读速度也不慢的话，都可以用，我们公司目前在HBase上的积累比较多了，所以技术选型会选择HBase，布隆过滤器的存储可以放在公司目前应用比较多的coherence中，也可以本地内存实现，这个重建速度比较快和占用内存比较小，放本地内存也挺合适。**
